{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fala dev! \n",
    "#### Nesse notebook nós vamos começar a entender como inicializar um projeto, como funciona um ambiente virtual e como instalar as bibliotecas necessárias para o nosso projeto. \n",
    "\n",
    "#### Iremos entender também como usar os tokens e já indicar para você o caminho para a proximas etapas. Então, vamos lá!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antes de começar a qualquer projeto, é interessante você criar um ambiente virtual. \n",
    "### Mas o que é um ambiente virtual?\n",
    "Ambiente virtual é um ambiente isolado do seu sistema operacional, onde você pode instalar as bibliotecas necessárias para o seu projeto sem interferir no seu sistema operacional.\n",
    "\n",
    "Olhe o exemplo abaixo de como instalar um ambiente virtual no seu sistema operacional:\n",
    "\n",
    "```bash\n",
    "pip install virtualenv\n",
    "```\n",
    "\n",
    "Feito isso, você pode criar um ambiente virtual com o comando abaixo:\n",
    "\n",
    "```bash\n",
    "virtualenv nome_do_seu_ambiente\n",
    "```\n",
    "Em geral chamamos o ambiente virtual de `env`, ou `venv`, mas você pode chamar do que quiser.\n",
    "\n",
    "Para ativar o ambiente virtual, você pode usar o comando abaixo:\n",
    "\n",
    "```bash\n",
    "cd nome_do_seu_ambiente/Scrips\n",
    ".\\activate\n",
    "```\n",
    "\n",
    "Se você observar o seu terminal, você verá que o nome do seu ambiente virtual aparecerá antes do seu nome de usuário.\n",
    "(Ex: (nome_do_seu_ambiente) C:\\Users\\seu_nome_de_usuario>)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "Bem agora que voçê já sabe o que é um ambiente virtual, vamos instalar as bibliotecas necessárias para o nosso projeto.\n",
    "\n",
    "Para isso muitas vezes usamos o arquivo `requirements.txt` que contém todas as bibliotecas necessárias para o nosso projeto.\n",
    "\n",
    "Obs: É importante que na construão do projeto a medida que você for instalando as bibliotecas, você vá atualizando o arquivo `requirements.txt`.\n",
    "\n",
    "Para instalar as bibliotecas necessárias para o nosso projeto, você pode usar o comando abaixo:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai (from -r requirements.txt (line 1))\n",
      "  Using cached langchain_openai-0.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting python-dotenv (from -r requirements.txt (line 2))\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting langchain-core<0.3,>=0.2.2 (from langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached langchain_core-0.2.7-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting openai<2.0.0,>=1.26.0 (from langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached openai-1.34.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached tiktoken-0.7.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached langsmith-0.1.77-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\talinho\\desktop\\nero\\nero-summer-24-2\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1)) (24.1)\n",
      "Collecting pydantic<3,>=1 (from langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Downloading tenacity-8.4.1-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting sniffio (from openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions<5,>=4.7 (from openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached regex-2024.5.15-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests>=2.26.0 (from tiktoken<1,>=0.7->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached orjson-3.10.5-cp312-none-win_amd64.whl.metadata (50 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.4 (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached pydantic_core-2.18.4-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai->-r requirements.txt (line 1))\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\talinho\\desktop\\nero\\nero-summer-24-2\\.venv\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1)) (0.4.6)\n",
      "Using cached langchain_openai-0.1.8-py3-none-any.whl (38 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached langchain_core-0.2.7-py3-none-any.whl (315 kB)\n",
      "Using cached openai-1.34.0-py3-none-any.whl (325 kB)\n",
      "Using cached tiktoken-0.7.0-cp312-cp312-win_amd64.whl (799 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langsmith-0.1.77-py3-none-any.whl (125 kB)\n",
      "Using cached pydantic-2.7.4-py3-none-any.whl (409 kB)\n",
      "Using cached pydantic_core-2.18.4-cp312-none-win_amd64.whl (1.9 MB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "Using cached regex-2024.5.15-cp312-cp312-win_amd64.whl (268 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading tenacity-8.4.1-py3-none-any.whl (27 kB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached orjson-3.10.5-cp312-none-win_amd64.whl (141 kB)\n",
      "Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "   ---------------------------------------- 0.0/121.4 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 112.6/121.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 121.4/121.4 kB 1.8 MB/s eta 0:00:00\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, tenacity, sniffio, regex, PyYAML, python-dotenv, orjson, jsonpointer, idna, h11, distro, charset-normalizer, certifi, annotated-types, requests, pydantic-core, jsonpatch, httpcore, anyio, tiktoken, pydantic, httpx, openai, langsmith, langchain-core, langchain_openai\n",
      "Successfully installed PyYAML-6.0.1 annotated-types-0.7.0 anyio-4.4.0 certifi-2024.6.2 charset-normalizer-3.3.2 distro-1.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.7 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.7 langchain_openai-0.1.8 langsmith-0.1.77 openai-1.34.0 orjson-3.10.5 pydantic-2.7.4 pydantic-core-2.18.4 python-dotenv-1.0.1 regex-2024.5.15 requests-2.32.3 sniffio-1.3.1 tenacity-8.4.1 tiktoken-0.7.0 tqdm-4.66.4 typing-extensions-4.12.2 urllib3-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "\n",
    "Para você conseguir acessar a API da OpenAI, você precisa de um token. Para conseguir um token, você precisa se cadastrar no site da OpenAI e solicitar um token, mas provavelmente o grade mestre senhor kaiô ja lhe passou seu token no privado.\n",
    "\n",
    "### Próximos passos\n",
    "Crie um arquivo `.env` e coloque o seu token lá. \n",
    "\n",
    "```bash\n",
    "OPENAI_API_KEY=seu_token\n",
    "```\n",
    "\n",
    "Agora que você já tem o seu ambiente virtual, já instalou as bibliotecas necessárias e já tem o seu token, você já está pronto para começar a codar... ou não."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agora vamos ver como o modelo fuciona via API\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deu errado? Não esqueceu de dar pip install nessa nova biblioteca que você quer utilizar?\n",
    "\n",
    "Já atualizou o requirements.txt hoje irmão?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender mais sobre ChatOpenAI, você pode acessar a documentação da OpenAI [aqui](https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos fazer uma pergunta\n",
    "question = \"Qual é a capital do Brasil?\"\n",
    "answer = llm.invoke(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos orientar resposta do modelo com um modelo de prompt \n",
    "# Esse modelo de prompt é uma string que é usada para orientar a resposta do modelo\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Voçê é um ajudante de aluno e está respondendo perguntas de geografia.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cadeia de comandos\n",
    "Chains of commands, ou cadeia de comandos, é uma sequência de comandos que são executados em sequência.\n",
    "\n",
    "Vamos combinar os comandos anteriores no que chamamos de chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos invocá-lo e fazer a mesma pergunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"Qual é a capital do Brasil?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A saída de um ChatModel (e, portanto, desta cadeia) é uma mensagem. No entanto, muitas vezes é muito mais conveniente trabalhar com strings. \n",
    "\n",
    "Vamos adicionar um analisador de saída simples para converter a mensagem de chat em uma string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"Qual é a capital do Brasil?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bem, essa é uma aplicação simples de cadeia de comandos. É possível criar cadeias de comandos mais complexas, \n",
    "\n",
    "com várias entradas e saídas, e até mesmo cadeias de comandos que chamam outras cadeias de comandos.\n",
    "\n",
    "Se aventure e crie suas chains pequeno garfanhoto.\n",
    "\n",
    "mais informações sobre chains [aqui](https://python.langchain.com/v0.1/docs/expression_language/cookbook/multiple_chains/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plus \n",
    "E se você quiser streamar a resposta do seu chatbot?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos usar o mesmo prompt para fazer outra pergunta e a resposta queremos que seja stream\n",
    "async for msg in chain.astream({\"input\": \"Qual é a capital do Brasil? Fale brevemente sobre a história do Brasil.\"}):\n",
    "    print(msg, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Então quer dizer que você já está streamando a resposta do seu chatbot?\n",
    "Já pode começar a ir pensando em como você aplicar isso em um projeto real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados\n",
    "\n",
    "Aqui estaremos carregando um texto sobre `Agents` de um blog no GitHub para exemplificação. Este foi um exemplo tirado da própria documentação do LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunks e Embeddings\n",
    "Nesse momento, entramos no foco desta etapa do tutorial que são: **chunks** e **embeddings**. Uma breve contextualização:\n",
    "\n",
    "- `Chunks`: são os pedaços de texto provenientes da divisão do seu documento, basicamente.\n",
    "- `Embeddings`: representações vetoriais númericas que incorporam semanticamente o conteúdo dos seus documentos (chunks).\n",
    "\n",
    "Seguiremos o tutorial utilizando as ferramentas do LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RecursiveCharacterTextSplitter\n",
    "\n",
    "A biblioteca fornece uma gama de `text_splitters`, que são classes/ferramentas usadas para conseguirmos realizar a divisão dos nossos textos e gerar os tão famosos chunks. O mais básico e mais usual deles é o `RecursiveCharacterTextSplitter`, recomendado para texto genérico. É parametrizado por uma lista de caracteres. Vamos dar uma olhada em como nós instanciamos esta classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos examinar os parâmetros definidos acima para o uso do **RecursiveCharacterTextSplitter**:\n",
    "\n",
    "- `chunk_size`: O tamanho máximo do chunk, onde o tamanho é determinado pela length_function. Geralmente, utilizamos a própria função `len`, então será o número máximo de caracteres por chunk. \n",
    "- `chunk_overlap`: Sobreposição entre os chunks. A sobreposição de blocos ajuda a mitigar a perda de informações quando o contexto é dividido entre blocos. Controla o quanto cada chunk consecutivo terá em comum com o bloco anterior.\n",
    "- `length_function`: Função que determina o tamanho do chunk.\n",
    "- `is_separator_regex`: Se a lista de separadores (padrão [\"\\n\\n\", \"\\n\", \" \", \"\"]) deve ser interpretada como regex.\n",
    "\n",
    "Agora que um ``text_splitter`` foi instanciado, podemos então gerar os nossos chunks. Com os parâmetros que usamos, dados o(s) nosso(s) documento(s), estaremos dividindo ele em chunks de 1000 caracteres no máximo, com sobreposição de 200 caracteres. Vamos ver a mágica acontecendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks), type(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você notar, a variável chunks é uma lista com 66 elementos. Esses elementos, conforme vemos abaixo, são instâncias da classe Document, assim como o seu documento carregado inicialmente. No entanto, agora temos mais porque os chunks são representações de menor tamanho do documento original, mas que o representam em sua totalidade.\n",
    "\n",
    "**Adendo**:\n",
    "Se tivéssemos uma string pura, ou um arquivo de texto lido diretamente, para gerarmos chunks dessa string pura precisamos utilizar um outro método:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_pura = \"\"\"ipsum lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, \n",
    "                quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. \n",
    "                Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod tempor \n",
    "                incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit \n",
    "                in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. ipsum lorem dolor \n",
    "                sit amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea\n",
    "                commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia \n",
    "                deserunt mollit anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud \n",
    "                exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat \n",
    "                cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore \n",
    "                magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore\n",
    "                eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do\n",
    "                eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in \n",
    "                reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. ipsum \n",
    "                lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam quis nostrud exercitation ullamco laboris nisi ut aliquip \n",
    "                ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident sunt in culpa qui officia \n",
    "                deserunt mollit anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam quis nostrud \n",
    "                exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat \n",
    "                cupidatat non proident sunt in culpa qui officia deserunt mollit anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore \n",
    "                magna aliqua. Ut enim ad minim veniam quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore \n",
    "                eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident sunt in culpa qui officia deserunt mollit anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do \n",
    "                eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in \n",
    "                reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident sunt in culpa qui officia deserunt mollit anim id est laborum. ipsum \n",
    "                lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam quis nostrud exercitation ullamco laboris nisi ut aliquip \n",
    "                ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident sunt in culpa qui officia \n",
    "                deserunt mollit anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam quis nostrud \n",
    "                exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fug ipsum lorem dolor sit amet, consectetur \n",
    "                adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. \n",
    "                Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit \n",
    "                anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation \n",
    "                ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat \n",
    "                non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna \n",
    "                aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore\n",
    "                eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit \n",
    "                sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure \n",
    "                dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est \n",
    "                laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco \n",
    "                laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non \n",
    "                proident, sunt in culpa qui officia deserunt mollit anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. \n",
    "                Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat \n",
    "                nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod \n",
    "                tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit \n",
    "                in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident sunt in culpa qui officia deserunt mollit anim id est laborum. ipsum lorem dolor sit \n",
    "                amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo \n",
    "                consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident sunt in culpa qui officia deserunt \n",
    "                mollit anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam quis nostrud exercitation \n",
    "                ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident \n",
    "                sunt in culpa qui officia deserunt mollit anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim \n",
    "                veniam quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur \n",
    "                sint occaecat cupidatat non proident sunt in culpa qui officia deserunt mollit anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et \n",
    "                dolore magna aliqua. Ut enim ad minim veniam quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore \n",
    "                eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident sunt in culpa qui officia deserunt mollit anim id est laborum. ipsum lorem dolor sit amet, consectetur adipiscing elit sed do eiusmod \n",
    "                tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in \n",
    "                voluptate velit esse cillum dolore eu fug\"\"\"\n",
    "\n",
    "chunks_2 = text_splitter.create_documents([string_pura]) # passamos dentro de uma lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Embeddings\n",
    "\n",
    "Agora, a ideia é que geremos `embeddings`, isto é, as representações vetoriais semânticas, dos chunks que acabamos de criar. Aqui, estaremos utilizando o modelo de embeddings da OpenAI para gerá-los e faremos a conexão ainda por meio da classe que o LangChain disponibiliza para realizarmos essa requisição de forma fácil, o `OpenAIEmbeddings`.\n",
    "\n",
    "Logo, temos:\n",
    "\n",
    "#### 1. Instancie a classe com sua credencial (sua API Token) e sete o modelo que você quer utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\", dimensions=1536)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerações:\n",
    "\n",
    "1. Passamos a chave para conectar com a API. Lembre que a cada requisição para a API da OpenAI, haverá algum custo associado por requisição.\n",
    "2. O modelo que utilizamos aqui foi o `text-embedding-3-small`, e é válido salientar que ele não é o único modelo de embedding da OpenAI. Em termos de desempenho e custo, ele está no intermedio, ou seja, não é o mais barato nem o mais caro, além de não ser o pior nem o melhor em questão de performance. Os outros que existem vou deixar como pesquisa, além de noções de custo.\n",
    "3. As incorporações são feitas em um espaço n-dimensional. Por padrão, para esse modelo utilizado, as incorporações são feitas em um espaço dimensional de 1536 dimensões, mas se olhar a documentação da OpenAI, a incorporação pode ser feita para outro número de dimensões e a ideia é a mesma para os outros modelos de embeddings da OpenAI. Vale a pesquisa de quais são.|\n",
    "\n",
    "#### 2. Gere os embeddings\n",
    "Quando vamos gerar embeddings, geramos embeddings de textos (conteúdos textuais, strings). Passamos como parâmetro de fato uma lista, mas uma lista de strings. Não podemos passar a lista dos chunks diretamente porque os elementos da lista são do tipo Document, logo, basta fazermos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_agent = embeddings.embed_documents([chunk.page_content for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_agent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings_agent), len(embeddings_agent[0])\n",
    "# quantos embeddings, quantas dimensões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E assim geramos embeddings dos nossos chunks/documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquisição externa de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para trabalhar em projetos reais, muitas vezes é necessário trabalhar com dados externos. Por isso, vamos fazer um exemplo com a API do Youtube integrada com o LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet  youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=xdfVomq3lAc\", add_video_info=False\n",
    ")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=xdfVomq3lAc\", add_video_info=True\n",
    ")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando Chunks\n",
    "\n",
    "Muitos textos são muito grandes quando saem de uma transcrição do vídeo. Para facilitar o processamento, é interessante dividir o texto em pedaços menores, nosso chunks que já vimos antes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opção 1: Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "chunks_split = text_splitter.split_documents(docs)\n",
    "chunks_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opção 2: Usando CHUNKS por TimeStamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.youtube import TranscriptFormat\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=xdfVomq3lAc\",\n",
    "    add_video_info=True,\n",
    "    transcript_format=TranscriptFormat.CHUNKS,\n",
    "    chunk_size_seconds=60,\n",
    ")\n",
    "chunks_transcript = loader.load()\n",
    "chunks_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks_transcript), len(chunks_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bem, parece que o chunks por text_splitter dicidiu mais a transcrição do texto. Se isso é melhor ou pior vai depender do problema que se está trabalhando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\", dimensions=128)\n",
    "embeddings_agent = embeddings.embed_documents([chunk.page_content for chunk in chunks_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_agent[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para amanhã\n",
    "\n",
    "1. Escolher duas playlists do youtube com temas distintos\n",
    "2. Faça o Embeddings dos vídeos\n",
    "3. Compare os embeddings dos vídeos das duas playlists em um gráfico bidimensional\n",
    "\n",
    "O que significa o posicionamentos desses chunks no gráfico? Como você pode interpretar isso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
